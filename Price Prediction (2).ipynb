{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:38:44.162598Z",
     "start_time": "2019-05-26T23:38:44.145254Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:38:45.536401Z",
     "start_time": "2019-05-26T23:38:44.276741Z"
    }
   },
   "outputs": [],
   "source": [
    "path = 'embedding_files/'\n",
    "\n",
    "max_embedding = pd.read_json(path+'max_embedding.json')\n",
    "min_embedding = pd.read_json(path+'min_embedding.json')\n",
    "mean_embedding = pd.read_json(path+'mean_embedding.json')\n",
    "sum_embedding = pd.read_json(path+'sum_embedding.json')\n",
    "\n",
    "djia = pd.read_csv('data/DJIA_table.csv')\n",
    "djia = djia.loc[:, ['Date', 'Open', 'Adj Close']].sort_values('Date').set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I only needed Date, Open, and Adj Close columns from the djia data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:38:45.560248Z",
     "start_time": "2019-05-26T23:38:45.543235Z"
    }
   },
   "outputs": [],
   "source": [
    "open_price = djia[['Open']]\n",
    "adj_close_price = djia[['Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:38:45.583538Z",
     "start_time": "2019-05-26T23:38:45.564590Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-08-08</th>\n",
       "      <td>11432.089844</td>\n",
       "      <td>11734.320312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-11</th>\n",
       "      <td>11729.669922</td>\n",
       "      <td>11782.349609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-12</th>\n",
       "      <td>11781.700195</td>\n",
       "      <td>11642.469727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-13</th>\n",
       "      <td>11632.809570</td>\n",
       "      <td>11532.959961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-08-14</th>\n",
       "      <td>11532.070312</td>\n",
       "      <td>11615.929688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Open     Adj Close\n",
       "Date                                  \n",
       "2008-08-08  11432.089844  11734.320312\n",
       "2008-08-11  11729.669922  11782.349609\n",
       "2008-08-12  11781.700195  11642.469727\n",
       "2008-08-13  11632.809570  11532.959961\n",
       "2008-08-14  11532.070312  11615.929688"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "djia.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:38:45.620754Z",
     "start_time": "2019-05-26T23:38:45.587089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-08-08</th>\n",
       "      <td>[0.809297204, 0.5163459778, 0.3755577505, 0.59...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          Max\n",
       "2008-08-08  [0.809297204, 0.5163459778, 0.3755577505, 0.59..."
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_embedding.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each value in the list is a feature, I redefined the dataframe by separating them into each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:38:45.636433Z",
     "start_time": "2019-05-26T23:38:45.625200Z"
    }
   },
   "outputs": [],
   "source": [
    "def transform_data(tbl):\n",
    "    \n",
    "    tbl = pd.DataFrame(tbl.iloc[:, 0].tolist())\n",
    "    tbl = tbl.set_index(djia.index)\n",
    "\n",
    "    return tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:38:47.826153Z",
     "start_time": "2019-05-26T23:38:45.640278Z"
    }
   },
   "outputs": [],
   "source": [
    "max_embedding = transform_data(max_embedding)\n",
    "\n",
    "min_embedding = transform_data(min_embedding)\n",
    "\n",
    "sum_embedding = transform_data(sum_embedding)\n",
    "\n",
    "mean_embedding = transform_data(mean_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:38:47.888134Z",
     "start_time": "2019-05-26T23:38:47.828345Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>758</th>\n",
       "      <th>759</th>\n",
       "      <th>760</th>\n",
       "      <th>761</th>\n",
       "      <th>762</th>\n",
       "      <th>763</th>\n",
       "      <th>764</th>\n",
       "      <th>765</th>\n",
       "      <th>766</th>\n",
       "      <th>767</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-08-08</th>\n",
       "      <td>0.809297</td>\n",
       "      <td>0.516346</td>\n",
       "      <td>0.375558</td>\n",
       "      <td>0.592091</td>\n",
       "      <td>0.372241</td>\n",
       "      <td>0.27578</td>\n",
       "      <td>0.672928</td>\n",
       "      <td>0.902444</td>\n",
       "      <td>1.321722</td>\n",
       "      <td>0.690093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414205</td>\n",
       "      <td>0.687436</td>\n",
       "      <td>0.144865</td>\n",
       "      <td>0.403365</td>\n",
       "      <td>0.304636</td>\n",
       "      <td>0.796824</td>\n",
       "      <td>0.586465</td>\n",
       "      <td>0.883279</td>\n",
       "      <td>0.854595</td>\n",
       "      <td>0.175066</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 768 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1         2         3         4        5    \\\n",
       "Date                                                                    \n",
       "2008-08-08  0.809297  0.516346  0.375558  0.592091  0.372241  0.27578   \n",
       "\n",
       "                 6         7         8         9    ...       758       759  \\\n",
       "Date                                                ...                       \n",
       "2008-08-08  0.672928  0.902444  1.321722  0.690093  ...  0.414205  0.687436   \n",
       "\n",
       "                 760       761       762       763       764       765  \\\n",
       "Date                                                                     \n",
       "2008-08-08  0.144865  0.403365  0.304636  0.796824  0.586465  0.883279   \n",
       "\n",
       "                 766       767  \n",
       "Date                            \n",
       "2008-08-08  0.854595  0.175066  \n",
       "\n",
       "[1 rows x 768 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_embedding.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:38:47.902602Z",
     "start_time": "2019-05-26T23:38:47.892407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1989, 768), (1989, 1), (1989, 1))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_embedding.shape, open_price.shape, adj_close_price.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I separated them into testing and training next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:38:47.916449Z",
     "start_time": "2019-05-26T23:38:47.906161Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_test(embedding, test_size):\n",
    "    \n",
    "    embedding_test = embedding.iloc[-test_size:, :]\n",
    "    embedding = embedding.iloc[:-test_size, :]\n",
    "    \n",
    "    return embedding_test, embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:38:47.952839Z",
     "start_time": "2019-05-26T23:38:47.919527Z"
    }
   },
   "outputs": [],
   "source": [
    "test_size = 300\n",
    "\n",
    "max_embedding_test, max_embedding = split_test(max_embedding, test_size)\n",
    "min_embedding_test, min_embedding = split_test(min_embedding, test_size)\n",
    "sum_embedding_test, sum_embedding = split_test(sum_embedding, test_size)\n",
    "mean_embedding_test, mean_embedding = split_test(mean_embedding, test_size)\n",
    "\n",
    "combined_embedding = pd.concat((mean_embedding, max_embedding, min_embedding, sum_embedding), axis=1)\n",
    "combined_embedding_test = pd.concat((mean_embedding_test, max_embedding_test, min_embedding_test, sum_embedding_test), axis=1)\n",
    "\n",
    "open_test, open_price = split_test(open_price, test_size)\n",
    "adj_close_test, adj_close_price = split_test(adj_close_price, test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:38:50.784333Z",
     "start_time": "2019-05-26T23:38:50.767288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1689, 768), (1689, 3072), (1689, 1), (1689, 1))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_embedding.shape, combined_embedding.shape, open_price.shape, adj_close_price.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>combined_embedding</b> is another dataset I tried to see if how using all of their features affect the open price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now there are total of 5 different models with different data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, below is my custom data loader and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T22:57:50.404182Z",
     "start_time": "2019-05-26T22:57:50.334600Z"
    }
   },
   "outputs": [],
   "source": [
    "def data_loader(data, batch_size, num_iter=100):\n",
    "    \n",
    "    # x : Embedding Values\n",
    "    # y : Open Price\n",
    "    # z : Close Price\n",
    "    \n",
    "    x = data[0]\n",
    "    y = data[1]\n",
    "    z = data[2]\n",
    "    \n",
    "    # num_iter iterations per epoch\n",
    "    # mini batch\n",
    "    \n",
    "    for _ in range(num_iter):\n",
    "    \n",
    "        idx = np.random.choice(np.arange(x.shape[0]), size=batch_size, replace=False)\n",
    "\n",
    "        batch_x = x.iloc[idx, :]\n",
    "        batch_y = y.iloc[idx]\n",
    "        batch_z = z.iloc[idx]\n",
    "        \n",
    "        yield batch_x, batch_y, batch_z\n",
    "        \n",
    "        \n",
    "class get_model():\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-3, dropout_rate=.5):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # BERT Embedding\n",
    "        self.x = tf.placeholder(tf.float32, shape=(None, 768))\n",
    "        # Open Price\n",
    "        self.y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        # Adj Close Price\n",
    "        self.z = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.pred = self.run_model()\n",
    "        \n",
    "        self.loss = tf.sqrt(tf.losses.mean_squared_error(self.z, self.pred), name='loss')\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='optimizer').minimize(self.loss)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def run_model(self):\n",
    "        \n",
    "        # Dense model\n",
    "        layer1 = tf.contrib.layers.fully_connected(self.x, 1000)\n",
    "        layer1 = tf.nn.dropout(layer1, rate=self.dropout_rate)\n",
    "        layer1 = tf.layers.batch_normalization(layer1)\n",
    "        \n",
    "        layer2 = tf.contrib.layers.fully_connected(layer1, 500)\n",
    "        layer2 = tf.nn.dropout(layer2, rate=self.dropout_rate)\n",
    "        layer2 = tf.layers.batch_normalization(layer2)\n",
    "        \n",
    "        # This would be the value of coefficient indicating how much it impacts on a day's open price\n",
    "        layer3 = tf.contrib.layers.fully_connected(layer2, 1)\n",
    "        \n",
    "        layer4 = layer3 * self.y\n",
    "        \n",
    "        layer5 = tf.contrib.layers.fully_connected(layer4, 100)\n",
    "        layer5 = tf.nn.dropout(layer5, rate=self.dropout_rate)\n",
    "        layer5 = tf.layers.batch_normalization(layer5)\n",
    "        \n",
    "        output = tf.contrib.layers.fully_connected(layer5, 1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From first to third layer is to extract the value that indicates how much given articles affect the same day's open price. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>get_data</b> below concatenates embedding with open price and split them into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:03:01.893010Z",
     "start_time": "2019-05-26T23:03:01.879465Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(embedding):\n",
    "    \n",
    "    X = pd.concat((embedding, open_price), axis=1)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, adj_close_price, test_size=.2)\n",
    "    \n",
    "    return [X_train.iloc[:, :-1], X_train.iloc[:, -1:], y_train], [X_valid.iloc[:, :-1], X_valid.iloc[:, -1:], y_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:03:02.752152Z",
     "start_time": "2019-05-26T23:03:02.492354Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_data_train, mean_data_valid = get_data(mean_embedding)\n",
    "max_data_train, max_data_valid = get_data(max_embedding)\n",
    "min_data_train, min_data_valid = get_data(min_embedding)\n",
    "sum_data_train, sum_data_valid = get_data(sum_embedding)\n",
    "\n",
    "combined_data_train, combined_data_valid = get_data(combined_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:05:29.713798Z",
     "start_time": "2019-05-26T23:05:29.627187Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_name = {'mean_embedding':[mean_data_train, mean_data_valid],\n",
    "            'max_embedding':[max_data_train, max_data_valid],\n",
    "            'min_embedding':[min_data_train, min_data_valid],\n",
    "            'sum_embedding':[sum_data_train, sum_data_valid]}\n",
    "\n",
    "def train_model(embedding_name, learning_rate=1e-5, epochs=300, batch_size=16, dropout_rate=.5, load_params=True,\n",
    "               verbose=True, save_model=True):\n",
    "    \n",
    "    data_train, data_valid = data_name[embedding_name]\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    model = get_model(learning_rate=learning_rate, dropout_rate=dropout_rate)\n",
    "\n",
    "    # For plots\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if load_params:\n",
    "        # Load Model\n",
    "            try:\n",
    "                print(f'------------- Attempting to Load {embedding_name} Model -------------')\n",
    "                model.saver.restore(sess, f'./model/{embedding_name}_model.ckpt')\n",
    "                print(f'------------- {embedding_name} Model Loaded -------------')\n",
    "            except:\n",
    "                print('Training New Model')\n",
    "        else:\n",
    "            print('Training New Model')\n",
    "\n",
    "        # Train Model\n",
    "        print('\\n------------- Training Model -------------\\n')\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for x, y, z in data_loader(data_train, batch_size=batch_size):\n",
    "\n",
    "                train_loss, _ = sess.run([model.loss, model.optimizer], feed_dict={model.x:x, \n",
    "                                                                             model.y:y, \n",
    "                                                                             model.z:z})\n",
    "\n",
    "            # x : embedding, y : open price, z : close price\n",
    "            valid_loss = sess.run(model.loss, feed_dict={model.x:data_valid[0], \n",
    "                                                         model.y:data_valid[1], \n",
    "                                                         model.z:data_valid[2]})\n",
    "\n",
    "            # print losses\n",
    "            if verbose:\n",
    "                print(f'Epoch {epoch+1}/{epochs},  Train RMSE Loss {train_loss}, Valid RMSE Loss {valid_loss}')\n",
    "\n",
    "                \n",
    "                \n",
    "            # Save Model at every 20 epochs\n",
    "            if save_model:\n",
    "                \n",
    "                if (epoch+1) % 20 == 0 and epoch > 0:\n",
    "                    if not os.path.exists('./model'):\n",
    "                        os.mkdir('./model/')\n",
    "\n",
    "                    model.saver.save(sess, f\"./model/{embedding_name}_model.ckpt\")\n",
    "                    print('\\n------------- Model Saved -------------\\n')\n",
    "                \n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            \n",
    "    return model, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:06:39.749656Z",
     "start_time": "2019-05-26T23:06:39.730376Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Possible Names : mean_embedding, max_embedding, min_embedding, sum_embedding\n",
    "epochs = 300\n",
    "learning_rate = 1e-4\n",
    "\n",
    "mean_model, mean_train_loss, mean_valid_loss = train_model('mean_embedding', epochs=epochs, learning_rate=learning_rate, load_params=False, verbose=False)\n",
    "max_model, max_train_loss, max_valid_loss = train_model('max_embedding', epochs=epochs, learning_rate=learning_rate, load_params=False, verbose=False)\n",
    "min_model, min_train_loss, min_valid_loss = train_model('min_embedding', epochs=epochs, learning_rate=learning_rate, load_params=False, verbose=False)\n",
    "sum_model, sum_train_loss, sum_valid_loss = train_model('sum_embedding', epochs=epochs, learning_rate=learning_rate, load_params=False, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running four models took about 25 minutes on surface pro 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Model for Combined Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the combined_embedding is in different shape, I created a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T23:11:13.698323Z",
     "start_time": "2019-05-26T23:11:13.666655Z"
    }
   },
   "outputs": [],
   "source": [
    "class combined_model():\n",
    "    \n",
    "    def __init__(self, learning_rate=1e-3, dropout_rate=.5):\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        # BERT Embedding\n",
    "        self.x = tf.placeholder(tf.float32, shape=(None, 3072))\n",
    "        # Open Price\n",
    "        self.y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "        # Adj Close Price\n",
    "        self.z = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "        self.pred = self.run_model()\n",
    "        \n",
    "        self.loss = tf.sqrt(tf.losses.mean_squared_error(self.z, self.pred), name='loss')\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='optimizer').minimize(self.loss)\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def run_model(self):\n",
    "        \n",
    "        # Dense layers\n",
    "        layer1 = tf.contrib.layers.fully_connected(self.x, 1000)\n",
    "        layer1 = tf.nn.dropout(layer1, rate=self.dropout_rate)\n",
    "        layer1 = tf.layers.batch_normalization(layer1)\n",
    "        \n",
    "        layer2 = tf.contrib.layers.fully_connected(layer1, 500)\n",
    "        layer2 = tf.nn.dropout(layer2, rate=self.dropout_rate)\n",
    "        layer2 = tf.layers.batch_normalization(layer2)\n",
    "        \n",
    "        # Coefficient of impact values\n",
    "        layer3 = tf.contrib.layers.fully_connected(layer2, 1)\n",
    "        \n",
    "        layer4 = layer3 * self.y\n",
    "        \n",
    "        layer5 = tf.contrib.layers.fully_connected(layer4, 100)\n",
    "        layer5 = tf.nn.dropout(layer5, rate=self.dropout_rate)\n",
    "        layer5 = tf.layers.batch_normalization(layer5)\n",
    "        \n",
    "        output = tf.contrib.layers.fully_connected(layer5, 1)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combined model took about 4 minutes on surface pro 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T21:36:21.916256Z",
     "start_time": "2019-05-03T21:32:30.742573Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Attempting to Load Combined Model -------------\n",
      "Training New Model\n",
      "\n",
      "------------- Training Model -------------\n",
      "\n",
      "Epoch 1/100,  Train RMSE Loss 14868.9697265625, Valid RMSE Loss 12097.6728515625\n",
      "\n",
      "------------- Model Saved -------------\n",
      "\n",
      "Epoch 21/100,  Train RMSE Loss 3311.416015625, Valid RMSE Loss 3776.328125\n",
      "\n",
      "------------- Model Saved -------------\n",
      "\n",
      "Epoch 41/100,  Train RMSE Loss 2456.6015625, Valid RMSE Loss 2350.877685546875\n",
      "\n",
      "------------- Model Saved -------------\n",
      "\n",
      "Epoch 61/100,  Train RMSE Loss 2424.5302734375, Valid RMSE Loss 2064.84033203125\n",
      "\n",
      "------------- Model Saved -------------\n",
      "\n",
      "Epoch 81/100,  Train RMSE Loss 1417.530029296875, Valid RMSE Loss 2210.999755859375\n",
      "\n",
      "------------- Model Saved -------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = combined_model(learning_rate=1e-4, dropout_rate=.5)\n",
    "epochs = 300\n",
    "\n",
    "combined_train_losses = []\n",
    "combined_valid_losses = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    try:\n",
    "        print(f'------------- Attempting to Load Combined Model -------------')\n",
    "        model.saver.restore(sess, f'./model/combined_model.ckpt')\n",
    "        print(f'------------- Combined Model Loaded -------------')\n",
    "        \n",
    "    except:\n",
    "        print('Training New Model')\n",
    "\n",
    "    # Train Model\n",
    "    print('\\n------------- Training Model -------------\\n')\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for x, y, z in data_loader(combined_data_train, batch_size=16):\n",
    "\n",
    "            train_loss, _ = sess.run([model.loss, model.optimizer], feed_dict={model.x:x, \n",
    "                                                                         model.y:y, \n",
    "                                                                         model.z:z})\n",
    "\n",
    "        valid_loss = sess.run(model.loss, feed_dict={model.x:combined_data_valid[0], \n",
    "                                                     model.y:combined_data_valid[1], \n",
    "                                                     model.z:combined_data_valid[2]})\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            \n",
    "            print(f'Epoch {epoch+1}/{epochs},  Combined Train RMSE Loss {train_loss}, Combined Valid RMSE Loss {valid_loss}')\n",
    "\n",
    "            if not os.path.exists('./model'):\n",
    "                os.mkdir('./model/')\n",
    "\n",
    "            model.saver.save(sess, f\"./model/combined_model.ckpt\")\n",
    "            print('\\n------------- Model Saved -------------\\n')\n",
    "\n",
    "        combined_train_losses.append(train_loss)\n",
    "        combined_valid_losses.append(valid_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Losses of each model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the results of loss plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Four Models' Losses](plots/4_losses.png)\n",
    "![Combined Loss](plots/combined_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T20:34:45.545725Z",
     "start_time": "2019-05-03T20:34:45.501679Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_data = {'mean_embedding':mean_embedding_test,\n",
    "                 'max_embedding':max_embedding_test,\n",
    "                 'min_embedding':min_embedding_test,\n",
    "                 'sum_embedding':sum_embedding_test}\n",
    "\n",
    "def predict_model(embedding_name):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    data = embedding_data[embedding_name]\n",
    "\n",
    "    model = get_model(learning_rate=1e-5)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #     Load Model\n",
    "        try:\n",
    "            print(f'------------- Attempting to Load {embedding_name} Model -------------')\n",
    "            model.saver.restore(sess, f'./model/{embedding_name}_model.ckpt')\n",
    "            print('------------- Model Loaded -------------')\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "        pred = sess.run(model.pred, feed_dict={model.x:data, \n",
    "                                                    model.y:open_test})\n",
    "        \n",
    "    return model, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T20:35:05.630224Z",
     "start_time": "2019-05-03T20:34:59.413690Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Attempting to Load mean_embedding Model -------------\n",
      "INFO:tensorflow:Restoring parameters from ./model/mean_embedding_model.ckpt\n",
      "------------- Model Loaded -------------\n",
      "------------- Attempting to Load max_embedding Model -------------\n",
      "INFO:tensorflow:Restoring parameters from ./model/max_embedding_model.ckpt\n",
      "------------- Model Loaded -------------\n",
      "------------- Attempting to Load sum_embedding Model -------------\n",
      "INFO:tensorflow:Restoring parameters from ./model/sum_embedding_model.ckpt\n",
      "------------- Model Loaded -------------\n",
      "------------- Attempting to Load min_embedding Model -------------\n",
      "INFO:tensorflow:Restoring parameters from ./model/min_embedding_model.ckpt\n",
      "------------- Model Loaded -------------\n"
     ]
    }
   ],
   "source": [
    "mean_model, mean_pred = predict_model('mean_embedding')\n",
    "max_model, max_pred = predict_model('max_embedding')\n",
    "sum_model, sum_pred = predict_model('sum_embedding')\n",
    "min_model, min_pred = predict_model('min_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T21:48:22.897336Z",
     "start_time": "2019-05-03T21:48:21.103206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Attempting to Load Combined Model -------------\n",
      "INFO:tensorflow:Restoring parameters from ./model/combined_model.ckpt\n",
      "------------- Model Loaded -------------\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = combined_model(learning_rate=1e-5)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#     Load Model\n",
    "    try:\n",
    "        print(f'------------- Attempting to Load Combined Model -------------')\n",
    "        model.saver.restore(sess, f'./model/combined_model.ckpt')\n",
    "        print('------------- Model Loaded -------------')\n",
    "\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    combined_pred = sess.run(model.pred, feed_dict={model.x:combined_embedding_test, \n",
    "                                                model.y:open_test})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-03T20:35:05.639733Z",
     "start_time": "2019-05-03T20:35:05.634009Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_pred = mean_pred.flatten()\n",
    "max_pred = max_pred.flatten()\n",
    "min_pred = min_pred.flatten()\n",
    "sum_pred = sum_pred.flatten()\n",
    "\n",
    "combined_pred = combined_pred.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Four Models' Predictions](plots/4_predictions.png)\n",
    "![Combined Prediction](plots/combined_prediction.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the predicted values have high variance and predicted values fluctuate much. However, the models still were able to capture general trend of the prices. As it is impossible to predict something with 100%, models like above are only used as a general guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to improve a model is to set a threshold which it limits how much the price can change over a day. For example, we can set it to 10,000 that it won't change above the amount. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the news I used may not (or most likely not) be related to DJIA. Using news that are closely related to it can also improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thank you for reading the post and if there is any mistake I made, please let me know!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
